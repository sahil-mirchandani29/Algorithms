I. PROBLEM
There are Trees and Graph’s that shows the relation between each attribute but where these relations came from and how their relations can benefit. So, I have used concept of trees and developed an algorithm of decision trees. In decision trees we pass a set of data and they help you to make classification decisions based on the data.
We have learnt about travelling salesman problem and how it can be solved by using various approaches such as Dynamic programming, greedy methods and backtracking but all of these algorithms require a distance matrix and number of nodes and distances between each of them specified which is not true in real life problems. I have solved this problem by implementing my own algorithm that would overcome some flaws in travelling salesman problem. My algorithm works on a real-world set of real time locations and we can add or remove any location and it would provide the shortest path to reach all the points with the non- polynomial complexity.

II. ALGORITHMS

A. DecisionTrees
In Decision Tree the major challenge is to identification of the attribute for the root node in each level. This process is known as attribute selection. We choose the method of finding Information Gain of every attribute at each level.
Information Gain is the entropy of the data for each attribute. Basically, the information that the attribute gives about the prediction label. The formula for finding Information gain is: Suppose S is a set of instances, A is an attribute, Sv is the subset of S with A = v, and Values (A) is the set of all possible values of A, then
Gain = entropy - submission(Sv/S*entropy(Sv)
Where Entropy is,
entropy= - submission(Pi * Log2pi) 
When we calculate Information Gain of each attribute, at each level we choose the attribute of highest Information Gain as It would be best for splitting data.
After we train the model on our data and calculate every split then we test the model on our test data set which provides the accuracy of the model.
Decision trees are famous for making Binary decisions such as weather would rain or sunny, person would buy or not buy a car etc. but decision trees over train on the model because of selecting numerous attributes that doesn’t take part in making better decisions.
Hence, this is the Algorithm for Creating decision trees from the data. This decision trees helps to make better prediction to the data available.


Algorithm:
ID3 (Examples, Target_Attribute, Attributes)
1. Create a root node for the tree
2. If all examples are positive, Return the single-node tree Root, with label = +.
3. If all examples are negative, Return the single-node tree Root, with label = -.
4. If number of predicting attributes is empty, then Return the single node tree Root, with label = most common value of the target attribute in the examples.
5. Otherwise Begin
    a) A ← The Attribute that best classifies examples. It is decided by finding Information Gain of every attribute and selecting the one with highest Gain
    b) Decision Tree attribute for Root = A.
For each possible value, (class)vi, of A,
    a) Add a new tree branch below Root, corresponding to the test A = vi.
    b) Let Examples(vi) be the subset of examples that have the value vi for A
If Examples(vi) is empty
    • Then below this new branch add a leaf node with label = most common target value in the examples
Else
End
Return Root


B. Travellingproblem

In travelling salesman problem, the main issue was the polynomial time complexity. The complexity of the n nodes is n! combinations to find the optimal route of n nodes. If we apply this algorithm on the real-world example with multiple address to find the shortest route between all the nodes. As there would be multiple address n! would take a large amount of time to compute the optimal distance. Hence algorithm would take a significant amount of time to compute the minimal distance between the points. It won’t provide the optimal solution, but the time taken by algorithm is significantly less than available solutions for travelling salesman problem.
The algorithm takes input of a text file containing all the address that a travelling man has to visit. The first address is the starting point for the algorithm. Then it takes all the address as the input and forms clusters with respect to the distance between them. We can manually set the cluster size to compute clusters. Cluster size is the accepted distance between the address. Algorithm takes O(kn) time to form clusters. Where k is the number of clusters. The worst case would be every point is farthest from each other or our cluster size is inappropriate according to the data input then it would take O(n2) complexity to take input and form k clusters. After cluster formation we sort the cluster according to their midpoints to visit the. For Example, we have 5 clusters then the order of travel could be [1,3,4,5,2] or etc. The algorithm would take O(k2) time to sort the clusters as we don’t have distances between midpoints of the clusters hence, We need to compute distance between them and then make an order of the clusters. After we have made an order of the cluster we need to sort the internal address of the cluster so as to visit the internal clusters in the minimal order possible.

Algorithm:
1. While read each line from the text file;
    a. If no clusters formed 
        i. Form cluster
    b. Else:
        i. Compute distance between the address and clusters midpoints
        ii. If its within cluster size then add address to the cluster
        iii. Else: make a new cluster with the address
2. Add the first cluster into sorted order list
3. For each midpoint of cluster:
    a. Compute the cluster midpoint with the shortest distance between the current_ midpoint and last element in the sorted order list
    b. Return the sorted order list
4. For each cluster in sorted order list:
    a. Sort the internal elements of the cluster
    b. Return sorted cluster
5. Make a result_list according to the sorted order list with sorted internal clusters.
6. Return result_list
